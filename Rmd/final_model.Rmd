---
title: "final_model"
author: "Tianyu Luo; Nanxin Li; Xuanbo Han"
output: pdf_document
geometry: margin=0.5in
urlcolor: blue
---

```{r, warning=FALSE, message=FALSE}
# If library imports failed, uncomment to install packages
# install.packages("car")
# install.packages("zoo")
# install.packages("stargazer")

# Library imports
library(tidyverse)
library(car)
library(zoo)
library(stargazer)

# load the dataset
# If you are using vs code, please remove the ../ from the path.
data <- read.csv("../data/most-streamed-spotify-songs-2024.csv") 
```

## Data Cleaning

```{r}
set.seed(123)  # for reproducibility
train_index <- sample(seq_len(nrow(data)), size = 0.7 * nrow(data))

train_data <- data[train_index, ]
test_data  <- data[-train_index, ]
```


```{r}
# Check data types
sapply(data, class)
```

response = Spotify Stream predictor = Spotify Playlist Count, Youtube Views, Youtube Likes, TikTok Posts, TikTok Views, Apple Music Playlist Count, Shazam Counts, AirPlay Spins, Explicit Track(0 = Clean, 1 = Explicit Content)

```{r}
# Remove spaces and special characters from column names (in case there is any)
names(data) <- gsub(" ", ".", names(data))
names(data) <- gsub("-", ".", names(data))

names(data)
```
# Preliminary Model
```{r}
# Take the features we need to use from the data table
features <- c("Spotify.Streams", "Spotify.Playlist.Count", "YouTube.Views", 
              "YouTube.Likes", "TikTok.Posts", "TikTok.Views", 
              "Apple.Music.Playlist.Count", "Shazam.Counts", "AirPlay.Spins")
```

```{r}
# From the above type check for each variable, every column is in character
# and numbers are in format xxx,xxx,xxx so commas in between must be removed
convert_to_numeric <- function(x) {
  # If already numeric, return the original value since no changes are needed
  if(is.numeric(x)) return(x)
  
  x <- gsub(",", "", x)           # Remove commas
  x <- gsub(" ", "", x)           # Remove spaces
  x <- trimws(x)                  # Remove leading/trailing whitespace
  
  # Convert to numeric
  x <- as.numeric(x)
  
  return(x)
}

for(col in features) {
  train_data[[col]] <- convert_to_numeric(train_data[[col]])
}
cat("Number of rows remaining:", nrow(train_data), "\n")
```

```{r}
# Check which rows have complete data for key variables
complete_rows <- complete.cases(train_data[, features])
# Keep only the rows with non-missing values for all features
spotify_clean <- train_data[complete_rows, ]

cat("Number of rows remaining:", nrow(spotify_clean), "\n")
```

Since after cleaning the dataset, we are remained with 2916 valid entries, we can still assume that we can train a valid model from this dataset.

```{r}
# Remove Duplicate data from dataset (prevent the same song being used twice in the model)
# Check for duplicate ISRCs since the same ISRC represent the same song
if("ISRC" %in% names(train_data)) {
  duplicate_isrc <- sum(duplicated(train_data$ISRC))
  cat("Songs with duplicate ISRC:", duplicate_isrc, "\n")
  
  # Remove duplicates by ISRC, keeping first occurrence
  spotify_clean <- spotify_clean[!duplicated(spotify_clean$ISRC), ]
}

predictors <- c("Spotify.Playlist.Count", "YouTube.Views", "YouTube.Likes", 
                "TikTok.Posts", "TikTok.Views", "Apple.Music.Playlist.Count", 
                "Shazam.Counts", "AirPlay.Spins")
```

```{r}
# For exporting the cleaned dataset
variables_to_keep <- c(features, "Explicit.Track")

# Create cleaned dataset with only needed variables
spotify_model_data <- spotify_clean[, variables_to_keep]

# Export to CSV
write.csv(spotify_model_data, "../data/spotify_cleaned.csv", row.names = FALSE)
```


## Exploratory Data Analysis

```{r}
# Create response + predictor summary table (not including explicit track - it will presented in a separate frequency table)
summary_table <- data.frame(
  Variable = character(),
  Min = numeric(),
  Q1 = numeric(),
  Median = numeric(),
  Mean = numeric(),
  Q3 = numeric(),
  Max = numeric(),
  stringsAsFactors = FALSE
)

for (f in features) {
  x <- spotify_clean[[f]]
  
  summary_table <- rbind(summary_table, data.frame(
    Feature = f,
    Min = min(x, na.rm = TRUE),
    Q1 = quantile(x, 0.25, na.rm = TRUE),
    Median = median(x, na.rm = TRUE),
    Mean = mean(x, na.rm = TRUE),
    Q3 = quantile(x, 0.75, na.rm = TRUE),
    Max = max(x, na.rm = TRUE)
  ))
}

# remove rownames to avoid quantile range being printed accidentally
rownames(summary_table) <- NULL

# Print table
print(summary_table)
```

```{r}
# reshape dataset to long format and add predictor layers for scatterplots
spotify_long <- spotify_clean %>%
  select(Spotify.Streams, all_of(predictors)) %>%
  pivot_longer(cols = all_of(predictors),
               names_to = "Predictor",
               values_to = "Value")
```

```{r}
# Create plot
ggplot(spotify_long, aes(x = Value, y = Spotify.Streams)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  facet_wrap(~ Predictor, scales = "free")
```

```{r}
# Create histograms
ggplot(spotify_long, aes(x = Value)) +
  geom_histogram(fill = "lightblue", color = "black", bins = 30) +
  facet_wrap(~ Predictor, scales = "free", ncol = 2) +
  labs(title = "Distribution of Each Predictor",
       y = "Frequency")

table(spotify_clean$Explicit.Track)
```

## Model Fitting

```{r}
preliminary_model <- lm(Spotify.Streams ~ Spotify.Playlist.Count + YouTube.Views + YouTube.Likes + TikTok.Posts + TikTok.Views + Apple.Music.Playlist.Count + Shazam.Counts + AirPlay.Spins + Explicit.Track + Spotify.Playlist.Count:Explicit.Track, data=spotify_clean)

summary(preliminary_model)
```

## Problematic Observations in Preliminary Model
```{r}
n <- nrow(spotify_clean)
p <- length(coef(preliminary_model))-1
hcut <- 2*(p + 1)/n
cookcut <- qf(0.5, df1=p+1, df2=n-p-1)
fitcut <- 2 * sqrt((p + 1) / n)
betacut <- 2 / sqrt(n)
```

```{r}
h_ii <- hatvalues(preliminary_model)
r_i <- rstandard(preliminary_model)
D_i <- cooks.distance(preliminary_model)
dffits_i <- dffits(preliminary_model)
dfbetas_i <- dfbetas(preliminary_model)
```


```{r}
# indices flagged by each rule
lev_idx      <- which(h_ii > hcut)                       # leverage points
reg_out_idx  <- which(r_i > 4 | r_i < -4)                # regression outliers
cook_idx     <- which(D_i > cookcut)                     # Cook's distance
dffits_idx   <- which(abs(dffits_i) > fitcut)            # DFFITS
dfbetas_idx  <- which(apply(abs(dfbetas_i) > betacut, 1, any))  # any beta
```

```{r}
all_influential_idx <- sort(unique(c(
  lev_idx,
  reg_out_idx,
  cook_idx,
  dffits_idx,
  dfbetas_idx
)))
```

```{r}
# proportion of influential points in spotify_valid
prop_influential <- length(all_influential_idx) / n

prop_influential
```

## Multicollinearity

```{r}
vif(preliminary_model)
```
The preliminary fitted model is $$Spotify.Playlist.Count = -22100000 + 4325\text{Spotify.Playlist Count} + 0.02179\text{YouTube.Views} + 11.7\text{YouTube.Likes} + 31.6\text{TikTok.Posts} + 0.02417\text{TikTok.Views} + 1682000\text{Apple.Music.Playlist.Count} + 8.444\text{Shazam.Counts} - 0.03575\text{AirPlay.Spins} + 7944000\text{Explicit.Track} - -90.08\text{Spotify.Playlist.Count}*{Explicit.Track}$$ Firstly, we check the conditional mean response condition to make sure that the observations from residual plots are conclusive. From each response vs. predictor plot, there is no obvious non-linear relationship from the plot, so the conditional mean response condition holds. Also, from scatter plot between each pair of predictor, no pair of predictors seem to exhibit a significant non-linear pattern, so the conditional mean predictor condition also holds.

Secondly, from the residual vs. fitted plot, a clear fanning pattern is present indicating a non-constant variance in the residuals. Also, the qqnorm plot demonstrates a non-linear pattern and largely deviates at the head and tail of the distribution. This indicates a violation in the normality assumption. In the observation of residual vs. fitted and each pair of residual vs. predictor, there is no obvious non-linear pattern or clustering presented, after examining the nature of each feature, there is likely no violation in the linearity and uncorrelated error assumption.

# Final Model
## Feature Engineering

```{r}
# Ensure correct format
spotify_clean$Release.Date <- as.Date(spotify_clean$Release.Date, format = "%d/%m/%Y")

# Check conversion
head(spotify_clean$Release.Date)

# Set baseline as the earliest release date
baseline_date <- min(spotify_clean$Release.Date, na.rm = TRUE)
baseline_date  # Optional: print baseline

# Compute time difference in months
spotify_clean$time <- as.numeric(
  (as.yearmon(spotify_clean$Release.Date) - as.yearmon(baseline_date)) * 12
)

# Convert to integer (if preferred)
spotify_clean$time <- round(spotify_clean$time)

# Check results
head(spotify_clean[, c("Release.Date", "time")])
```

Clean Dataset
```{r}
# Remove observations with missing timestamp (either Release.Date or time is NA)
spotify_time <- spotify_clean[!is.na(spotify_clean$Release.Date) & !is.na(spotify_clean$time), ]

# Check number of remaining observations
nrow(spotify_time)

# Optional: view first few cleaned rows
head(spotify_time[, c("Release.Date", "time")])
```

```{r}
time_model <- lm(Spotify.Streams ~ time + Spotify.Playlist.Count + YouTube.Views + YouTube.Likes + TikTok.Posts + TikTok.Views + Apple.Music.Playlist.Count + Shazam.Counts + AirPlay.Spins + Explicit.Track + Spotify.Playlist.Count:Explicit.Track, data=spotify_time)
summary(time_model)
```

```{r}
vif(time_model)
```

```{r}
reduced_time_model <- lm(Spotify.Streams ~ time + YouTube.Views + YouTube.Likes + TikTok.Posts + TikTok.Views + Apple.Music.Playlist.Count + Shazam.Counts + AirPlay.Spins + Explicit.Track + Spotify.Playlist.Count:Explicit.Track, data=spotify_time)
summary(reduced_time_model)
```

```{r}
reduced_time_model2 <- lm(Spotify.Streams ~ time + YouTube.Views + YouTube.Likes + TikTok.Posts + Apple.Music.Playlist.Count + Shazam.Counts + AirPlay.Spins + Explicit.Track + Spotify.Playlist.Count:Explicit.Track, data=spotify_time)
summary(reduced_time_model2)
```

```{r}
vif(reduced_time_model2)
```

```{r}
reduced_time_model3 <- lm(Spotify.Streams ~ time + YouTube.Views + YouTube.Likes + TikTok.Posts + Apple.Music.Playlist.Count + Shazam.Counts + Explicit.Track + Spotify.Playlist.Count:Explicit.Track, data=spotify_time)
summary(reduced_time_model3)
```

```{r, echo=F, fig.width=8, fig.height=11}
e_hat <- resid(reduced_time_model3)
y_hat <- fitted(reduced_time_model3)

feature_indices <- match(features, names(spotify_time))

par(mfrow=c(4,3))
plot(spotify_time$Spotify.Streams ~ y_hat, ylab="Spotify Streams", xlab="Fitted")
plot(e_hat ~ y_hat, ylab="Residuals", xlab="Fitted")
for(i in feature_indices){
 plot(e_hat ~ spotify_time[,i], ylab="Residuals", xlab=names(spotify_time)[i])
}
plot(e_hat, ylab="Residuals", xlab="Observation number")
qqnorm(e_hat)
qqline(e_hat)
```

The above model has decently large $R^2$ and adjusted $R^2$, but there are clearly assumption violations

```{r}
# boxCox transformation applied to response variable
bc <- boxCox(reduced_time_model3)

yLambda <- bc$x[which.max(bc$y)]

# Apply power transformation to predictor variables
tfs <- powerTransform(cbind(spotify_time[,feature_indices]))
summary(tfs)
```

```{r}
# Add transformed column
spotify_time$tf_streams <- (spotify_time$Spotify.Streams)**(2/5)
```


```{r}
tf_reduced_time_model1 <- lm(tf_streams ~ time + Spotify.Playlist.Count + YouTube.Likes + TikTok.Posts + Apple.Music.Playlist.Count, data=spotify_time)
summary(tf_reduced_time_model1)
```

```{r}
tf_features <- c("tf_streams", "time", "Spotify.Playlist.Count",
              "YouTube.Likes", "TikTok.Posts", 
              "Apple.Music.Playlist.Count")
```


```{r, echo=F, fig.width=8, fig.height=11}
e_hat <- resid(tf_reduced_time_model1)
y_hat <- fitted(tf_reduced_time_model1)

feature_indices <- match(tf_features, names(spotify_time))

par(mfrow=c(4,3))
plot(e_hat ~ y_hat, ylab="Residuals", xlab="Fitted")
for(i in feature_indices){
 plot(e_hat ~ spotify_time[,i], ylab="Residuals", xlab=names(spotify_time)[i])
}
plot(e_hat, ylab="Residuals", xlab="Observation number")
qqnorm(e_hat)
qqline(e_hat)
```

Normality violation is mitigated but non-linearity and non-constant variance are still present

```{r}
# boxCox transformation applied to response variable
bc <- boxCox(tf_reduced_time_model1)

X <- spotify_time[, feature_indices]

# Shift features if needed
for (j in seq_along(X)) {
  min_val <- min(X[, j], na.rm = TRUE)
  if (min_val <= 0) {
    X[, j] <- X[, j] + abs(min_val) + 1  # Shift to make strictly positive
  }
}

# Apply power transformation
tfs <- powerTransform(X)
summary(tfs)
```

```{r}
# Add sqrt-transformed column
spotify_time$tf_time <- (spotify_time$time) ** 5

# Optional: view first few rows to verify
head(spotify_time[, c("time", "tf_time")])
```

```{r}
tf_reduced_time_model2 <- lm(tf_streams ~ tf_time + Spotify.Playlist.Count + YouTube.Likes + TikTok.Posts + Apple.Music.Playlist.Count, data=spotify_time)
summary(tf_reduced_time_model2)
```

```{r}
tf_features <- c("tf_streams", "tf_time", "Spotify.Playlist.Count",
              "YouTube.Likes", "TikTok.Posts", 
              "Apple.Music.Playlist.Count")
```


```{r, echo=F, fig.width=8, fig.height=11}
e_hat <- resid(tf_reduced_time_model2)
y_hat <- fitted(tf_reduced_time_model2)

feature_indices <- match(tf_features, names(spotify_time))

par(mfrow=c(4,3))
plot(e_hat ~ y_hat, ylab="Residuals", xlab="Fitted")
for(i in feature_indices){
 plot(e_hat ~ spotify_time[,i], ylab="Residuals", xlab=names(spotify_time)[i])
}
plot(e_hat, ylab="Residuals", xlab="Observation number")
qqnorm(e_hat)
qqline(e_hat)
```

non-constant variance are slightly mitigated, but non-linearity are still present. Apply more transformations

```{r}
# Add transformed column
spotify_time$tf_youtube <- (spotify_time$YouTube.Likes)**(0.3)
spotify_time$tf_tiktok <- (spotify_time$TikTok.Posts)**(0.2)
spotify_time$tf_apple <- (spotify_time$Apple.Music.Playlist.Count)**(0.2)
```

```{r}
tf_reduced_time_model3 <- lm(tf_streams ~ tf_time + Spotify.Playlist.Count + tf_youtube + tf_tiktok + Apple.Music.Playlist.Count + tf_time:Spotify.Playlist.Count, data=spotify_time)
summary(tf_reduced_time_model3)
```

```{r}
tf_features <- c("tf_streams", "tf_time", "Spotify.Playlist.Count",
              "tf_youtube", "tf_tiktok", 
              "tf_apple")
```


```{r, echo=F, fig.width=8, fig.height=11}
e_hat <- resid(tf_reduced_time_model3)
y_hat <- fitted(tf_reduced_time_model3)

feature_indices <- match(tf_features, names(spotify_time))

par(mfrow=c(4,3))
plot(e_hat ~ y_hat, ylab="Residuals", xlab="Fitted")
for(i in feature_indices){
 plot(e_hat ~ spotify_time[,i], ylab="Residuals", xlab=names(spotify_time)[i])
}
plot(e_hat, ylab="Residuals", xlab="Observation number")
qqnorm(e_hat)
qqline(e_hat)
```

```{r}
n <- nrow(spotify_time)
p <- length(coef(tf_reduced_time_model3))-1
hcut <- 2*(p + 1)/n
cookcut <- qf(0.5, df1=p+1, df2=n-p-1)
fitcut <- 2 * sqrt((p + 1) / n)
betacut <- 2 / sqrt(n)
```

```{r}
h_ii <- hatvalues(tf_reduced_time_model3)
r_i <- rstandard(tf_reduced_time_model3)
D_i <- cooks.distance(tf_reduced_time_model3)
dffits_i <- dffits(tf_reduced_time_model3)
dfbetas_i <- dfbetas(tf_reduced_time_model3)
```


```{r}
# indices flagged by each rule
lev_idx      <- which(h_ii > hcut)                       # leverage points
reg_out_idx  <- which(r_i > 4 | r_i < -4)                # regression outliers
cook_idx     <- which(D_i > cookcut)                     # Cook's distance
dffits_idx   <- which(abs(dffits_i) > fitcut)            # DFFITS
dfbetas_idx  <- which(apply(abs(dfbetas_i) > betacut, 1, any))  # any beta
```

```{r}
all_influential_idx <- sort(unique(c(
  lev_idx,
  reg_out_idx,
  cook_idx,
  dffits_idx,
  dfbetas_idx
)))
```

```{r}
# proportion of influential points
prop_influential <- length(all_influential_idx) / n

prop_influential
```

```{r}
# index of most influential point among all observations
most_inf_idx <- which.max(D_i)

most_inf_idx
```

After verifying this entry is incorrectly labelled, so is thus removed

```{r}
spotify_time <- spotify_time[-most_inf_idx, ]
```

```{r}
tf_reduced_time_model4 <- lm(tf_streams ~ tf_time + Spotify.Playlist.Count + tf_youtube + tf_tiktok + Apple.Music.Playlist.Count + tf_time:Spotify.Playlist.Count, data=spotify_time)
summary(tf_reduced_time_model3)
```

```{r}
tf_features <- c("tf_streams", "tf_time", "Spotify.Playlist.Count",
              "tf_youtube", "tf_tiktok", 
              "Apple.Music.Playlist.Count")
```

```{r, echo=F, fig.width=8, fig.height=11}
library(ggplot2)

# 1. Setup Data
plot_data <- spotify_time
plot_data$Residuals <- resid(tf_reduced_time_model4)
plot_data$Fitted <- fitted(tf_reduced_time_model4)

# 2. Define Labels for ALL variables (Original + Transformed)
axis_labels <- c(
  # Original / Untransformed
  "time" = "Time",
  "Spotify.Playlist.Count" = "Spotify Playlist Count",
  "YouTube.Likes" = "YouTube Likes",
  "TikTok.Posts" = "TikTok Posts",
  "Apple.Music.Playlist.Count" = "Apple Music Playlist Count",
  
  # Transformed
  "tf_time" = "Time^(0.3)",
  "tf_youtube" = "YouTube Likes^(0.2)",
  "tf_tiktok" = "TikTok Posts^(0.2)"
)

# 3. Define the Order of Variables
# List 1: Original / Untransformed Variables
raw_vars <- c("time", "Spotify.Playlist.Count", "YouTube.Likes", 
              "TikTok.Posts", "Apple.Music.Playlist.Count")

# List 2: Transformed Variables (Only the ones that changed)
tf_vars  <- c("tf_time", "tf_youtube", "tf_tiktok")

# Initialize list and counters
my_plots <- list()
fig_main <- 2  
sub_count <- 1 

# --- A. Residuals vs Fitted ---
p <- ggplot(plot_data, aes(x = Fitted, y = Residuals)) +
  geom_point(alpha = 0.5, color = "black") +
  labs(
    x = "Fitted Values", 
    y = "Residuals",
    title = paste0("Figure ", fig_main, ".", sub_count, ": Residuals vs Fitted")
  ) +
  theme_bw()

my_plots[["Fitted"]] <- p
sub_count <- sub_count + 1

# --- B. Loop for UNTRANSFORMED Predictors ---
for(var in raw_vars){
  nice_label <- ifelse(var %in% names(axis_labels), axis_labels[var], var)
  
  p <- ggplot(plot_data, aes_string(x = var, y = "Residuals")) +
    geom_point(alpha = 0.5) +
    labs(
      x = nice_label, 
      y = "Residuals",
      title = paste0("Figure ", fig_main, ".", sub_count, ": Resid vs ", nice_label)
    ) +
    theme_bw() +
    theme(plot.title = element_text(size=10))
  
  my_plots[[var]] <- p
  sub_count <- sub_count + 1
}

# --- C. Loop for TRANSFORMED Predictors ---
for(var in tf_vars){
  nice_label <- ifelse(var %in% names(axis_labels), axis_labels[var], var)
  
  p <- ggplot(plot_data, aes_string(x = var, y = "Residuals")) +
    geom_point(alpha = 0.5, color = "blue") + # Optional: Blue to distinguish transformed
    labs(
      x = nice_label, 
      y = "Residuals",
      title = paste0("Figure ", fig_main, ".", sub_count, ": Resid vs ", nice_label)
    ) +
    theme_bw() +
    theme(plot.title = element_text(size=10))
  
  my_plots[[var]] <- p
  sub_count <- sub_count + 1
}

# --- D. Normal Q-Q Plot ---
p_qq <- ggplot(plot_data, aes(sample = Residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(
    x = "Theoretical Quantiles", 
    y = "Sample Quantiles",
    title = paste0("Figure ", fig_main, ".", sub_count, ": Normal Q-Q Plot")
  ) +
  theme_bw()

my_plots[["QQ"]] <- p_qq
```

```{r, echo=F, fig.width=4, fig.height=3}
for(p in my_plots) print(p)
```

```{r}
vif(tf_reduced_time_model4)
```

# Visualization for Poster

```{r}
library(ggplot2)
library(dplyr)

# --- 1. Data Preparation Function ---
prepare_combined_variance <- function(model, model_name) {
  # Extract Data
  y_obs <- model$model[[1]]  # The actual response values used in model
  residuals <- resid(model)
  
  # Center residuals around the mean of Y so they overlap correctly
  y_mean <- mean(y_obs)
  residuals_shifted <- residuals + y_mean
  
  # Create Data Frames
  df_total <- data.frame(
    Value = y_obs, 
    Type = "Total Variation (Data)", 
    Model = model_name
  )
  
  df_resid <- data.frame(
    Value = residuals_shifted, 
    Type = "Unexplained Variation (Residuals)", 
    Model = model_name
  )
  
  # Combine
  rbind(df_total, df_resid)
}

# --- 2. Generate Data for Both Models ---
# Note: Ensure 'preliminary_model' and 'tf_reduced_time_model4' are in your environment
data_prelim <- prepare_combined_variance(preliminary_model, "1. Preliminary Model\n(Raw Scale)")
data_final  <- prepare_combined_variance(tf_reduced_time_model4, "2. Final Model\n(Transformed Scale)")

# Combine into one plotting dataset
plot_data_combined <- rbind(data_prelim, data_final)

# Set Factor Levels to control layering order 
# (We want Total on bottom, Residuals on top)
plot_data_combined$Type <- factor(plot_data_combined$Type, 
                                  levels = c("Total Variation (Data)", "Unexplained Variation (Residuals)"))

# --- 3. Create the Overlapped Plot ---
ggplot(plot_data_combined, aes(x = "Distribution", y = Value)) +
  
  # Layer 1: The Total Variation (The Background)
  geom_violin(data = subset(plot_data_combined, Type == "Total Variation (Data)"),
              aes(fill = Type), 
              alpha = 0.3,          # Lighter transparency
              color = NA,           # No border for the background
              scale = "width",      # Maximize width
              width = 0.8) +        # Make it wide
  
  # Layer 2: The Unexplained Variation (The Foreground)
  geom_violin(data = subset(plot_data_combined, Type == "Unexplained Variation (Residuals)"),
              aes(fill = Type), 
              alpha = 0.8,          # More solid
              color = "black",      # Add border for definition
              scale = "width",      # Maximize width
              width = 0.4) +        # Make it narrower so it sits "inside"
  
  # Faceting: Side-by-Side Comparison with Independent Scales
  facet_wrap(~ Model, scales = "free_y") +
  
  # Colors
  scale_fill_manual(values = c("Total Variation (Data)" = "#FFC107",       # Gold
                               "Unexplained Variation (Residuals)" = "#00B0FF")) + # Blue
  
  # Labels and Theme
  labs(
    title = "Variance Decomposition: Model Improvement",
    subtitle = "Gold Area = Explained Variance (RÂ²); Blue Area = Unexplained Variance",
    y = "Response Variable Distribution",
    x = "",
    fill = ""
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_blank(), # Hide X axis text since we have legend
    strip.text = element_text(size = 11, face = "bold"),
    panel.grid.major.x = element_blank()
  )
```

```{r}
# Define the predictors used in the Final Model
# Note: Based on the model formula, Apple Music Count is used in its raw form, not transformed.
final_predictors <- c("tf_time", "Spotify.Playlist.Count", 
                      "tf_youtube", "tf_tiktok", "Apple.Music.Playlist.Count")

# Create a subset of the data with readable labels
plot_data <- spotify_time[, final_predictors]
colnames(plot_data) <- c("Time^(0.3)", "Spotify\nPlaylists", 
                         "YouTube\nLikes^(0.2)", "TikTok\nPosts^(0.2)", "Apple Music\nPlaylists")

# Generate Scatterplot Matrix
# 1. Define a transparent blue color using base R functions
# rgb(red, green, blue, alpha) where alpha is 0-1
my_color <- rgb(0, 0, 0.55, alpha = 0.4)

# 2. Run the pairs plot
pairs(plot_data, 
      main = "Predictor vs. Predictor Relationships (Final Model)",
      pch = 19, 
      col = my_color,
      lower.panel = NULL)
```

```{r}
library(ggplot2)
library(broom)
library(dplyr)

# 1. Extract coefficients and confidence intervals
# 'conf.int = TRUE' calculates the 95% CI automatically
model_data <- tidy(tf_reduced_time_model4, conf.int = TRUE) %>%
  filter(term != "(Intercept)")

# 2. Standardize the Estimates and CIs
# This scales them by SD_x / SD_y so they are comparable
sd_y <- sd(tf_reduced_time_model4$model[[1]])
sd_x <- apply(model.matrix(tf_reduced_time_model4)[, -1], 2, sd)

# Match the SDs to the terms in the dataframe
# We use match() to ensure the order is correct even if rows were reordered
scaling_factors <- (sd_x[model_data$term] / sd_y)

model_data <- model_data %>%
  mutate(
    std_estimate = estimate * scaling_factors,
    std_conf.low = conf.low * scaling_factors,
    std_conf.high = conf.high * scaling_factors
  )

# 3. Create the Plot
ggplot(model_data, aes(x = std_estimate, y = reorder(term, abs(std_estimate)))) +
  # Point representing the coefficient estimate
  geom_point(size = 3, color = "#2E86C1") +
  
  # Error bars representing the 95% Confidence Interval
  geom_errorbarh(aes(xmin = std_conf.low, xmax = std_conf.high), height = 0.2, color = "#2E86C1") +
  
  # Vertical line at 0 (Significance threshold)
  geom_vline(xintercept = 0, linetype = "dashed", color = "red") +
  
  labs(
    title = "Standardized Coefficients with 95% Confidence Intervals",
    subtitle = "Final Model",
    x = "Standardized Effect on Streams (Beta)",
    y = "Predictor"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 10, face = "bold"),
    panel.grid.major.y = element_blank() # Cleaner look
  )
```

```{r}
# --- K-Fold Cross Validation (K=10) ---

set.seed(302) # Ensure reproducibility

# 1. Setup
k <- 10
n <- nrow(spotify_time)
folds <- sample(rep(1:k, length.out = n)) # Assign each row to a random fold 1-10
cv_rmse <- numeric(k) # Store RMSE for each fold
cv_r2   <- numeric(k) # Store R^2 for each fold

# 2. The Loop
for(i in 1:k) {
  
  # a. Split data
  test_indices <- which(folds == i)
  cv_test_data <- spotify_time[test_indices, ]
  cv_train_data <- spotify_time[-test_indices, ]
  
  # b. Fit model on the training folds (SAME formula as your final model)
  # Formula: tf_streams ~ tf_time + Spotify.Playlist.Count + tf_youtube + tf_tiktok + Apple.Music.Playlist.Count + tf_time:Spotify.Playlist.Count
  cv_model <- lm(tf_streams ~ tf_time + Spotify.Playlist.Count + tf_youtube + 
                 tf_tiktok + Apple.Music.Playlist.Count + 
                 tf_time:Spotify.Playlist.Count, 
                 data = cv_train_data)
  
  # c. Predict on the test fold
  preds <- predict(cv_model, newdata = cv_test_data)
  actuals <- cv_test_data$tf_streams
  
  # d. Calculate Metrics for this fold
  rss <- sum((actuals - preds)^2)
  tss <- sum((actuals - mean(actuals))^2)
  
  cv_rmse[i] <- sqrt(mean((actuals - preds)^2)) # RMSE
  cv_r2[i]   <- 1 - (rss / tss)                 # R-squared
}

# 3. Output Results
cat("--- 10-Fold Cross Validation Results ---\n")
cat(sprintf("Average RMSE: %.4f (SD: %.4f)\n", mean(cv_rmse), sd(cv_rmse)))
cat(sprintf("Average R^2:  %.4f (SD: %.4f)\n", mean(cv_r2), sd(cv_r2)))

# 4. Visualization (Optional but great for the poster)
# Shows stability of the model across different data subsets
par(mfrow=c(1,2))
boxplot(cv_rmse, main="CV RMSE Distribution", col="lightblue", ylab="RMSE (Transformed Units)")
boxplot(cv_r2, main="CV R-Squared Distribution", col="lightgreen", ylab="R-Squared")
```

```{r}
library(zoo) # Required for as.yearmon

# --- 1. Define Cleaning Function ---
clean_numeric_col <- function(x) {
  x <- as.character(x)         # Ensure string
  x <- gsub(",", "", x)        # Remove commas
  x <- gsub(" ", "", x)        # Remove spaces
  as.numeric(x)                # Convert to number
}

# --- 2. Clean Test Data ---
# Create a copy so we don't overwrite the original
test_data_clean <- test_data

# List of columns that need comma removal
cols_to_clean <- c("Spotify.Streams", "Spotify.Playlist.Count", 
                   "YouTube.Likes", "TikTok.Posts", 
                   "Apple.Music.Playlist.Count")

# Apply cleaning loop
for(col in cols_to_clean) {
  if(col %in% names(test_data_clean)) {
    test_data_clean[[col]] <- clean_numeric_col(test_data_clean[[col]])
  }
}

# --- 3. Handle Date & Time Feature ---
# Convert Release.Date to Date object
test_data_clean$Release.Date <- as.Date(test_data_clean$Release.Date, format = "%d/%m/%Y")

# Recalculate 'time' (Months since baseline)
# CRITICAL: We must use the SAME baseline_date as the training set.
# If 'baseline_date' isn't in your environment, we recalculate it from the full 'data' or 'spotify_clean'
if(!exists("baseline_date")) {
  # Try to find it from the training set object if available
  if(exists("spotify_time")) {
     baseline_date <- min(spotify_time$Release.Date, na.rm = TRUE)
  } else {
     # Fallback (Just in case)
     baseline_date <- min(as.Date(data$Release.Date, format="%d/%m/%Y"), na.rm=TRUE)
  }
}

# Calculate time difference
test_data_clean$time <- as.numeric((as.yearmon(test_data_clean$Release.Date) - 
                                    as.yearmon(baseline_date)) * 12)
test_data_clean$time <- round(test_data_clean$time)

# --- 4. Remove NAs ---
# We filter out rows that have NAs in the specific columns our model uses
vars_needed <- c("Spotify.Streams", "time", "Spotify.Playlist.Count", 
                 "YouTube.Likes", "TikTok.Posts", "Apple.Music.Playlist.Count")

test_data_clean <- test_data_clean[complete.cases(test_data_clean[, vars_needed]), ]

# --- 5. Apply Final Model Transformations ---
# These must MATCH EXACTLY the transformations used in 'tf_reduced_time_model4'

# Response Transformation
test_data_clean$tf_streams <- (test_data_clean$Spotify.Streams)^(2/5)

# Predictor Transformations
test_data_clean$tf_time    <- (test_data_clean$time)^0.3
test_data_clean$tf_youtube <- (test_data_clean$YouTube.Likes)^0.2
test_data_clean$tf_tiktok  <- (test_data_clean$TikTok.Posts)^0.2
# Note: Apple.Music.Playlist.Count was NOT transformed in your final model formula

cat("Test Data Prepared. Valid rows:", nrow(test_data_clean))
```


```{r}
# 1. Fit the EXACT SAME model structure on the Test Data
# We do this solely to compare coefficients in the table (as seen in your image)
model_test_fit <- lm(tf_streams ~ tf_time + Spotify.Playlist.Count + tf_youtube + 
                     tf_tiktok + Apple.Music.Playlist.Count + 
                     tf_time:Spotify.Playlist.Count, 
                     data = test_data_clean) 
                     # Ensure 'test_data_clean' has your tf_ variables created!

# 2. Generate the Side-by-Side Table
stargazer(tf_reduced_time_model4, model_test_fit,
          
          # Labels for the columns
          column.labels = c("Final Model (Training)", "Final Model (Testing)"),
          dep.var.labels = "Transformed Spotify Streams",
          
          # Make labels readable (Replace these with your preferred names)
          covariate.labels = c(
            "Time (Transformed)", 
            "Spotify Playlist Count", 
            "YouTube Likes (Transformed)", 
            "TikTok Posts (Transformed)", 
            "Apple Music Playlist Count", 
            "Interaction: Time * Spotify Playlists",
            "Constant"
          ),
          
          # Statistics to display (matching your image)
          keep.stat = c("n", "rsq", "adj.rsq", "ser", "f"),
          
          # Output type
          type = "text", # Change to "latex" or "html" when knitting to PDF/HTML
          
          # Formatting
          digits = 3,
          star.cutoffs = c(0.05, 0.01, 0.001),
          
          # Title
          title = "Table 3: Regression Output Table for Final Model Fit to Training vs. Testing Data"
)
```

```{r}
library(car)
library(knitr)
library(dplyr)

# --- 1. Define the Diagnostics Function ---
get_model_diagnostics <- function(model, model_name) {
  
  # Basic Stats
  summ <- summary(model)
  n <- nrow(model$model)
  p <- length(coef(model)) - 1  # Number of predictors (excluding intercept)
  
  # 1. Information Criteria
  # Note: AIC/BIC are only comparable if Response and N are identical. 
  # We calculate them for completeness, but comparisons across transformed Y are invalid.
  aic_val <- AIC(model)
  bic_val <- BIC(model)
  aicc_val <- aic_val + (2 * (p + 1) * (p + 2)) / (n - p - 2) # AICc Formula
  
  # 2. VIF Violations (> 5)
  vif_val <- tryCatch(vif(model), error = function(e) NULL)
  vif_violation <- 0
  if (!is.null(vif_val)) {
    # Handle GVIF (for models with interactions/factors) vs Standard VIF
    if (is.matrix(vif_val)) {
      # For GVIF, we usually check GVIF column (column 1)
      vif_violation <- sum(vif_val[, 1] > 5)
    } else {
      vif_violation <- sum(vif_val > 5)
    }
  }
  
  # 3. Diagnostic Cutoffs
  h_cut <- 2 * (p + 1) / n
  cook_cut <- qf(0.5, df1 = p + 1, df2 = n - p - 1)
  fit_cut <- 2 * sqrt((p + 1) / n)
  beta_cut <- 2 / sqrt(n)
  
  # 4. Calculate Diagnostic Counts
  h_ii <- hatvalues(model)
  r_i <- rstandard(model)
  D_i <- cooks.distance(model)
  dffits_i <- dffits(model)
  dfbetas_i <- dfbetas(model)
  
  # Indices
  lev_idx <- which(h_ii > h_cut)
  outlier_idx <- which(abs(r_i) > 4) # Using > 4 as "Extreme" outlier (match image logic)
  cook_idx <- which(D_i > cook_cut)
  dffits_idx <- which(abs(dffits_i) > fit_cut)
  
  # DFBETAS: Check if ANY coefficient exceeds cutoff for an observation
  dfbetas_idx <- which(apply(abs(dfbetas_i) > beta_cut, 1, any))
  
  # 5. Unique Influential Points (Union of all issues)
  all_issues <- unique(c(lev_idx, outlier_idx, cook_idx, dffits_idx, dfbetas_idx))
  n_unique <- length(all_issues)
  pct_unique <- round((n_unique / n) * 100, 1)
  
  # Return Data Row
  data.frame(
    Metric_Name = model_name,
    N_Predictors = p,
    Adj_R2 = round(summ$adj.r.squared, 4),
    AIC = round(aic_val, 2),
    BIC = round(bic_val, 2),
    AICc = round(aicc_val, 2),
    VIF_Violations = vif_violation,
    N_Leverage = length(lev_idx),
    N_Outliers = length(outlier_idx),
    N_Cooks = length(cook_idx),
    N_DFFITS = length(dffits_idx),
    N_DFBETAS = length(dfbetas_idx),
    Unique_Influential = paste0(n_unique, " (", pct_unique, "%)")
  )
}

# --- 2. Apply to Your Models ---
# Make sure these model objects exist in your environment before running this chunk

# List of models to compare (Update names if yours are different)
# We create a list to handle potential errors if a model isn't found
models_to_check <- list()

if(exists("preliminary_model")) models_to_check[["Preliminary"]] <- preliminary_model
if(exists("reduced_time_model3")) models_to_check[["Reduced Linear"]] <- reduced_time_model3
if(exists("tf_reduced_time_model1")) models_to_check[["Transformed Y"]] <- tf_reduced_time_model1
if(exists("tf_reduced_time_model4")) models_to_check[["Final Model"]] <- tf_reduced_time_model4

# Generate Table
results_df <- data.frame()

for (name in names(models_to_check)) {
  res <- get_model_diagnostics(models_to_check[[name]], name)
  results_df <- rbind(results_df, res)
}

# --- 3. Transpose and Format Table (To match screenshot layout) ---
final_table <- t(results_df)
colnames(final_table) <- results_df$Metric_Name
final_table <- final_table[-1, ] # Remove the Name row since it's now header

# Display using kable
kable(final_table, caption = "Table 2: Model Diagnostics for Training and Testing Models")
```


```{r}
library(knitr)

# Select the variables actually used in your final model
# Note: We select the TRANSFORMED versions (tf_) where applicable
vars_of_interest <- c("tf_streams", "tf_time", "Spotify.Playlist.Count", 
                      "tf_youtube", "tf_tiktok", "Apple.Music.Playlist.Count")

# Create the summary table
knitr::kable(summary(spotify_time[, vars_of_interest]), 
             caption = "Summary Table of Variables in Final Model")
```

```{r}
attach(spotify_time) # Temporarily attach to make plotting easier
par(mfrow=c(2,3))    # Set up a 2x3 grid

# Plot histograms for your specific model variables
hist(tf_streams, breaks=20, main="Spotify Streams^(2/5) (Y)", col="lightblue")
hist(tf_time, breaks=20, main="Time^(0.3)", col="grey")
hist(Spotify.Playlist.Count, breaks=20, main="Spotify Playlists", col="grey")
hist(tf_youtube, breaks=20, main="YouTube Likes^(0.2)", col="grey")
hist(tf_tiktok, breaks=20, main="TikTok Posts^(0.2)", col="grey")
hist(Apple.Music.Playlist.Count, breaks=20, main="Apple Music Playlists Count", col="grey")

detach(spotify_time) # Always detach after use to avoid conflicts
```


```{r}
library(ggplot2)
library(gridExtra) # You might need: install.packages("gridExtra")

# Plot A: Streams vs Spotify Playlists (Curation)
p1 <- ggplot(data=spotify_time, aes(x=Spotify.Playlist.Count, y=tf_streams)) + 
  geom_point(alpha=0.3, color="darkblue") + 
  geom_smooth(method = lm, se = FALSE, color="red") + 
  labs(x = 'Spotify Playlists', y='Transformed Streams', 
       title = 'Curation: Playlists vs Streams') +
  theme_minimal()

# Plot B: Streams vs TikTok Posts (Virality)
p2 <- ggplot(data=spotify_time, aes(x=tf_tiktok, y=tf_streams)) + 
  geom_point(alpha=0.3, color="darkgreen") + 
  geom_smooth(method = lm, se = FALSE, color="red") + 
  labs(x = 'Transformed TikTok Posts', y='Transformed Streams', 
       title = 'Virality: TikTok vs Streams') +
  theme_minimal()

# Arrange them side by side
grid.arrange(p1, p2, nrow=1)
```

```{r}
library(knitr)

# --- 1. Calculate MSE for the Final Model (Training Data) ---
# We use the residuals directly from your existing final model
train_residuals <- resid(tf_reduced_time_model4)
mse_train_fit <- mean(train_residuals^2)

# --- 2. Train a New Model on the Test Data ---
# We fit a model using the EXACT SAME formula but on 'test_data_clean'
# This tells us: "How well does this equation fit the test data if we optimized it specifically for the test data?"
model_test_fit <- lm(tf_streams ~ tf_time + Spotify.Playlist.Count + tf_youtube + 
                     tf_tiktok + Apple.Music.Playlist.Count + 
                     tf_time:Spotify.Playlist.Count, 
                     data = test_data_clean)

# Calculate MSE for this new test-specific model
test_residuals <- resid(model_test_fit)
mse_test_fit <- mean(test_residuals^2)

# --- 3. Compare the Results ---
mse_comparison <- data.frame(
  Dataset = c("Training", "Test"),
  MSE = c(mse_train_fit, mse_test_fit),
  RMSE = c(sqrt(mse_train_fit), sqrt(mse_test_fit)) # RMSE is often easier to interpret
)

# Output the table
kable(mse_comparison, 
      digits = 4, 
      caption = "Table 6: Comparison of Model Fit (MSE) on Training vs. Test Populations")
```


